{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74082f4-4972-4fe4-9056-51075a2345c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, sys, time, os, math, tqdm, numpy, soundfile, time, pickle\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6122ca8-a200-4ed7-ae96-66740256a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel(self, audio):\n",
    "        audio_norm = audio / 32768.0\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
    "        melspec = torch.squeeze(melspec, 0).T\n",
    "        '''\n",
    "         mel = (D, T)\n",
    "        '''\n",
    "        #cut and pad\n",
    "        n_frames = melspec.shape[0]\n",
    "        p = self.target_lenght - n_frames\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            melspec = m(melspec)\n",
    "        elif p < 0:\n",
    "            melspec = melspec[0:self.target_lenght, :]       \n",
    "            \n",
    "        return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e45c45e-8125-4e00-af36-560a4cc8df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_network(eval_list, eval_path):\n",
    "\t\tself.eval()\n",
    "\t\tsetfiles = []\n",
    "\t\tembeddings = {}\n",
    "\t\tlines = open(eval_list).read().splitlines()\n",
    "\t\tfor line in lines:\n",
    "\t\t\tsetfiles.append([line.split()[1], line.split()[2]])\n",
    "\n",
    "\t\tfor idx, [file1, file2] in tqdm.tqdm(enumerate(setfiles), total = len(setfiles)):\n",
    "\t\t\taudio1, _  = self.load_wav_to_torch(os.path.join(eval_path, file1))\n",
    "\t\t\tmel1 = self.get_mel(audio1).unsqueeze(0).cuda()\n",
    "\t\t\taudio2, _  = self.load_wav_to_torch(os.path.join(eval_path, file2))\n",
    "\t\t\tmel2 = self.get_mel(audio2).unsqueeze(0).cuda()\n",
    "  \n",
    "\t\t\t# Speaker embeddings\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tembedding_1 = self.speaker_encoder.forward(mel1, aug = False)\n",
    "\t\t\t\tembedding_1 = F.normalize(embedding_1, p=2, dim=1)\n",
    "\t\t\t\tembedding_2 = self.speaker_encoder.forward(mel2, aug = False)\n",
    "\t\t\t\tembedding_2 = F.normalize(embedding_2, p=2, dim=1)\n",
    "\t\t\tembeddings[file] = [embedding_1, embedding_2]\n",
    "\t\tscores, labels  = [], []\n",
    "\n",
    "\t\tfor line in lines:\t\t\t\n",
    "\t\t\tembedding_11, embedding_12 = embeddings[line.split()[1]]\n",
    "\t\t\tembedding_21, embedding_22 = embeddings[line.split()[2]]\n",
    "\t\t\t# Compute the scores\n",
    "\t\t\tscore_1 = torch.mean(torch.matmul(embedding_11, embedding_21.T)) # higher is positive\n",
    "\t\t\tscore_2 = torch.mean(torch.matmul(embedding_12, embedding_22.T))\n",
    "\t\t\tscore = (score_1 + score_2) / 2\n",
    "\t\t\tscore = score.detach().cpu().numpy()\n",
    "\t\t\tscores.append(score)\n",
    "\t\t\tlabels.append(int(line.split()[0]))\n",
    "\t\t\t\n",
    "\t\t# Coumpute EER and minDCF\n",
    "\t\tEER = tuneThresholdfromScore(scores, labels, [1, 0.1])[1]\n",
    "\t\tfnrs, fprs, thresholds = ComputeErrorRates(scores, labels)\n",
    "\t\tminDCF, _ = ComputeMinDcf(fnrs, fprs, thresholds, 0.05, 1, 1)\n",
    "\n",
    "\t\treturn EER, minDCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9add3895-2144-43d3-997a-c23d0fcd869b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_network(self, eval_list, eval_path):\n",
    "\t\tself.eval()\n",
    "\t\tfiles = []\n",
    "\t\tembeddings = {}\n",
    "\t\tlines = open(eval_list).read().splitlines()\n",
    "\t\tfor line in lines:\n",
    "\t\t\tfiles.append(line.split()[1])\n",
    "\t\t\tfiles.append(line.split()[2])\n",
    "\t\tsetfiles = list(set(files))\n",
    "\t\tsetfiles.sort()\n",
    "\n",
    "\t\tfor idx, file in tqdm.tqdm(enumerate(setfiles), total = len(setfiles)):\n",
    "\t\t\taudio, _  = soundfile.read(os.path.join(eval_path, file))\n",
    "\t\t\t# Full utterance\n",
    "\t\t\tdata_1 = torch.FloatTensor(numpy.stack([audio],axis=0)).cuda()\n",
    "\n",
    "\t\t\t# Spliited utterance matrix\n",
    "\t\t\tmax_audio = 300 * 160 + 240\n",
    "\t\t\tif audio.shape[0] <= max_audio:\n",
    "\t\t\t\tshortage = max_audio - audio.shape[0]\n",
    "\t\t\t\taudio = numpy.pad(audio, (0, shortage), 'wrap')\n",
    "\t\t\tfeats = []\n",
    "\t\t\tstartframe = numpy.linspace(0, audio.shape[0]-max_audio, num=5)\n",
    "\t\t\tfor asf in startframe:\n",
    "\t\t\t\tfeats.append(audio[int(asf):int(asf)+max_audio])\n",
    "\t\t\tfeats = numpy.stack(feats, axis = 0).astype(numpy.float)\n",
    "\t\t\tdata_2 = torch.FloatTensor(feats).cuda()\n",
    "\t\t\t# Speaker embeddings\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tembedding_1 = self.speaker_encoder.forward(data_1, aug = False)\n",
    "\t\t\t\tembedding_1 = F.normalize(embedding_1, p=2, dim=1)\n",
    "\t\t\t\tembedding_2 = self.speaker_encoder.forward(data_2, aug = False)\n",
    "\t\t\t\tembedding_2 = F.normalize(embedding_2, p=2, dim=1)\n",
    "\t\t\tembeddings[file] = [embedding_1, embedding_2]\n",
    "\t\tscores, labels  = [], []\n",
    "\n",
    "\t\tfor line in lines:\t\t\t\n",
    "\t\t\tembedding_11, embedding_12 = embeddings[line.split()[1]]\n",
    "\t\t\tembedding_21, embedding_22 = embeddings[line.split()[2]]\n",
    "\t\t\t# Compute the scores\n",
    "\t\t\tscore_1 = torch.mean(torch.matmul(embedding_11, embedding_21.T)) # higher is positive\n",
    "\t\t\tscore_2 = torch.mean(torch.matmul(embedding_12, embedding_22.T))\n",
    "\t\t\tscore = (score_1 + score_2) / 2\n",
    "\t\t\tscore = score.detach().cpu().numpy()\n",
    "\t\t\tscores.append(score)\n",
    "\t\t\tlabels.append(int(line.split()[0]))\n",
    "\t\t\t\n",
    "\t\t# Coumpute EER and minDCF\n",
    "\t\tEER = tuneThresholdfromScore(scores, labels, [1, 0.1])[1]\n",
    "\t\tfnrs, fprs, thresholds = ComputeErrorRates(scores, labels)\n",
    "\t\tminDCF, _ = ComputeMinDcf(fnrs, fprs, thresholds, 0.05, 1, 1)\n",
    "\n",
    "\t\treturn EER, minDCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "703cb5a3-78f7-408d-b6fd-e72a092554fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39304 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m EER, minDCF \u001b[38;5;241m=\u001b[39m \u001b[43meval_network\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset/filelist_Vox1_test.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/root/autodl-tmp/vox1_test/wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36meval_network\u001b[0;34m(eval_list, eval_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \tsetfiles\u001b[38;5;241m.\u001b[39mappend([line\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m1\u001b[39m], line\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, [file1, file2] \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(setfiles), total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(setfiles)):\n\u001b[0;32m---> 10\u001b[0m \taudio1, _  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mload_wav_to_torch(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(eval_path, file1))\n\u001b[1;32m     11\u001b[0m \tmel1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mel(audio1)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#.cuda()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \taudio2, _  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_wav_to_torch(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(eval_path, file2))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "EER, minDCF = eval_network('Dataset/filelist_Vox1_test.txt', '/root/autodl-tmp/vox1_test/wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f44921-b4e1-4ab3-93c5-99b067c6713f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
